{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Twitter Sentiment Analysis Project\n",
        "\n",
        "Here is the complete project walkthrough with all the Python code. You can copy and paste this entire code block into a single Google Colab cell and run it.\n",
        "\n",
        "### Step 1: Setup, Load Data, and Explore\n",
        "\n",
        "First, we import our libraries and load a suitable dataset from Kaggle. This dataset conveniently contains tweets already labeled as `Positive`, `Negative`, and `Neutral`, just like in your project description.\n",
        "\n",
        "### Step 2: Text Preprocessing (NLTK)\n",
        "\n",
        "This is where we clean the text data. We'll create a function that performs the exact steps you listed:\n",
        "\n",
        "1.  **Clean Text:** Remove URLs, hashtags, mentions, and non-alphanumeric characters.\n",
        "2.  **Tokenize:** Split the text into individual words.\n",
        "3.  **Remove Stopwords:** Filter out common words (`\"the\"`, `\"is\"`, `\"in\"`) that don't add meaning.\n",
        "4.  **Lemmatize:** Reduce words to their root form (\"running\" -\\> \"run\", \"studies\" -\\> \"study\").\n",
        "\n",
        "### Step 3: Feature Engineering (Scikit-learn)\n",
        "\n",
        "A machine learning model can't read words. We must convert the cleaned text into a numerical format. We will use **TF-IDF** (Term Frequency-Inverse Document Frequency), a standard and effective method.\n",
        "\n",
        "  * **TF (Term Frequency):** How often a word appears in a single tweet.\n",
        "  * **IDF (Inverse Document Frequency):** How \"important\" a word is. Words that appear in *every* tweet (like \"tweet\") are less important than words that appear in only a few (like \"awesome\").\n",
        "\n",
        "### Step 4: Model Training and Evaluation\n",
        "\n",
        "This is the machine learning part.\n",
        "\n",
        "1.  **Split Data:** We'll use 80% of our data to *train* the model and 20% to *test* it.\n",
        "2.  **Train Model:** We'll use a **Multinomial Naive Bayes** classifier. It's a classic, fast, and highly effective model for text classification.\n",
        "3.  **Evaluate:** We'll check the model's performance on the unseen test data and print the **accuracy score** and a detailed **classification report**.\n",
        "\n",
        "-----\n",
        "\n",
        "## Complete Python Code for Google Colab"
      ],
      "metadata": {
        "id": "L_CsGYvwn4jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# --- 1. SETUP AND DATA LOADING ---\n",
        "\n",
        "# Download necessary NLTK data (stopwords, tokenizer, lemmatizer)\n",
        "# You only need to run these lines once\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab') # Download the missing punkt_tab resource\n",
        "\n",
        "# Load the dataset\n",
        "# Make sure you have uploaded 'twitter_training.csv' to your Colab session!\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "df = pd.read_csv('twitter_training.csv')\n",
        "\n",
        "# The dataset has 4 columns. We only need the tweet text and the sentiment label.\n",
        "# The column names are 'tweet' and 'label' respectively.\n",
        "# Let's rename them for clarity.\n",
        "df = df[['tweet', 'label']].rename(columns={'tweet': 'text', 'label': 'sentiment'})\n",
        "\n",
        "# Drop rows with missing text\n",
        "df = df.dropna(subset=['text'])\n",
        "\n",
        "# Map sentiments to numerical values (optional, but good practice for some models)\n",
        "# We'll also drop 'Irrelevant' to match the Positive/Negative/Neutral goal\n",
        "df = df[df['sentiment'] != 'Irrelevant']\n",
        "df['sentiment_label'] = df['sentiment'].map({'Positive': 2, 'Neutral': 1, 'Negative': 0})\n",
        "\n",
        "print(\"Data loaded and cleaned. Head:\")\n",
        "print(df.head())\n",
        "print(\"\\nSentiment distribution:\")\n",
        "print(df['sentiment'].value_counts())\n",
        "\n",
        "\n",
        "# --- 2. TEXT PREPROCESSING (NLTK) ---\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Ensure the input is a string\n",
        "    if not isinstance(text, str):\n",
        "        return \"\" # Return empty string for non-string inputs\n",
        "\n",
        "    # 1. Clean: Remove URLs, mentions, hashtags, and non-alphanumeric chars\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'@\\w+', '', text)     # Remove mentions\n",
        "    text = re.sub(r'#\\w+', '', text)     # Remove hashtags\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text) # Remove punctuation/numbers\n",
        "\n",
        "    # 2. Tokenize\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # 3. Remove Stopwords and Lemmatize\n",
        "    cleaned_tokens = []\n",
        "    for token in tokens:\n",
        "        if token not in stop_words:\n",
        "            cleaned_tokens.append(lemmatizer.lemmatize(token))\n",
        "\n",
        "    # 4. Join back into a string\n",
        "    return \" \".join(cleaned_tokens)\n",
        "\n",
        "print(\"\\nStarting preprocessing...\")\n",
        "# Apply the preprocessing function to our text column\n",
        "df['processed_text'] = df['text'].apply(preprocess_text)\n",
        "print(\"Preprocessing complete. Processed text example:\")\n",
        "print(df[['text', 'processed_text']].head())\n",
        "\n",
        "\n",
        "# --- 3. FEATURE ENGINEERING (SCIKIT-LEARN) ---\n",
        "\n",
        "print(\"\\nStarting feature engineering (TF-IDF)...\")\n",
        "\n",
        "# Initialize the TF-IDF Vectorizer\n",
        "# max_features=5000 means we only keep the top 5000 most important words\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "# Create the TF-IDF matrix (our 'X' features)\n",
        "X = tfidf_vectorizer.fit_transform(df['processed_text'])\n",
        "\n",
        "# Our 'y' labels\n",
        "y = df['sentiment_label']\n",
        "\n",
        "print(\"TF-IDF matrix shape:\", X.shape)\n",
        "\n",
        "\n",
        "# --- 4. MODEL TRAINING AND EVALUATION ---\n",
        "\n",
        "print(\"\\nSplitting data and training model...\")\n",
        "\n",
        "# Split the data: 80% for training, 20% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Initialize the Multinomial Naive Bayes classifier\n",
        "model = MultinomialNB()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Model trained.\")\n",
        "\n",
        "\n",
        "# --- 5. EVALUATION ---\n",
        "\n",
        "print(\"\\nEvaluating model performance...\")\n",
        "\n",
        "# Make predictions on the unseen test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\n--- Model Accuracy ---\")\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Print the detailed classification report\n",
        "print(\"\\n--- Classification Report ---\")\n",
        "target_names = ['Negative', 'Neutral', 'Positive'] # Must match the order of our labels (0, 1, 2)\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "\n",
        "\n",
        "# --- 6. HOW TO USE THE MODEL (Example) ---\n",
        "\n",
        "print(\"\\n--- Testing Model on New Tweets ---\")\n",
        "new_tweets = [\n",
        "    \"This is the best movie I have ever seen! Amazing.\",\n",
        "    \"I'm not sure how I feel about this product, it's just okay.\",\n",
        "    \"What a terrible experience. I will never buy this again.\",\n",
        "    \"The airline lost my luggage, I am so angry!\"\n",
        "]\n",
        "\n",
        "# 1. Preprocess the new tweets\n",
        "processed_new_tweets = [preprocess_text(tweet) for tweet in new_tweets]\n",
        "\n",
        "# 2. Transform using the *same* TF-IDF vectorizer\n",
        "new_tweets_tfidf = tfidf_vectorizer.transform(processed_new_tweets)\n",
        "\n",
        "# 3. Predict\n",
        "predictions = model.predict(new_tweets_tfidf)\n",
        "\n",
        "# Map predictions back to labels\n",
        "predicted_labels = [target_names[p] for p in predictions]\n",
        "\n",
        "for i in range(len(new_tweets)):\n",
        "    print(f\"Tweet: '{new_tweets[i]}'\")\n",
        "    print(f\"Predicted Sentiment: {predicted_labels[i]}\\n\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded and cleaned. Head:\n",
            "                                                text sentiment  \\\n",
            "0  im getting on borderlands and i will murder yo...  Positive   \n",
            "1  I am coming to the borders and I will kill you...  Positive   \n",
            "2  im getting on borderlands and i will kill you ...  Positive   \n",
            "3  im coming on borderlands and i will murder you...  Positive   \n",
            "4  im getting on borderlands 2 and i will murder ...  Positive   \n",
            "\n",
            "   sentiment_label  \n",
            "0                2  \n",
            "1                2  \n",
            "2                2  \n",
            "3                2  \n",
            "4                2  \n",
            "\n",
            "Sentiment distribution:\n",
            "sentiment\n",
            "Negative    22358\n",
            "Positive    20655\n",
            "Neutral     18108\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Starting preprocessing...\n",
            "Preprocessing complete. Processed text example:\n",
            "                                                text  \\\n",
            "0  im getting on borderlands and i will murder yo...   \n",
            "1  I am coming to the borders and I will kill you...   \n",
            "2  im getting on borderlands and i will kill you ...   \n",
            "3  im coming on borderlands and i will murder you...   \n",
            "4  im getting on borderlands 2 and i will murder ...   \n",
            "\n",
            "                 processed_text  \n",
            "0  im getting borderland murder  \n",
            "1            coming border kill  \n",
            "2    im getting borderland kill  \n",
            "3   im coming borderland murder  \n",
            "4  im getting borderland murder  \n",
            "\n",
            "Starting feature engineering (TF-IDF)...\n",
            "TF-IDF matrix shape: (61121, 5000)\n",
            "\n",
            "Splitting data and training model...\n",
            "Model trained.\n",
            "\n",
            "Evaluating model performance...\n",
            "\n",
            "--- Model Accuracy ---\n",
            "Accuracy: 71.72%\n",
            "\n",
            "--- Classification Report ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.70      0.82      0.75      4472\n",
            "     Neutral       0.76      0.56      0.64      3622\n",
            "    Positive       0.72      0.74      0.73      4131\n",
            "\n",
            "    accuracy                           0.72     12225\n",
            "   macro avg       0.72      0.71      0.71     12225\n",
            "weighted avg       0.72      0.72      0.71     12225\n",
            "\n",
            "\n",
            "--- Testing Model on New Tweets ---\n",
            "Tweet: 'This is the best movie I have ever seen! Amazing.'\n",
            "Predicted Sentiment: Positive\n",
            "\n",
            "Tweet: 'I'm not sure how I feel about this product, it's just okay.'\n",
            "Predicted Sentiment: Positive\n",
            "\n",
            "Tweet: 'What a terrible experience. I will never buy this again.'\n",
            "Predicted Sentiment: Negative\n",
            "\n",
            "Tweet: 'The airline lost my luggage, I am so angry!'\n",
            "Predicted Sentiment: Negative\n",
            "\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZwUU_T4n4jp",
        "outputId": "3d51b571-d986-4fd5-ec64-b63bf045ec6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to Achieve 85% Accuracy\n",
        "\n",
        "The code above will likely yield an accuracy between 70% and 80%. Reaching the **85%** accuracy mentioned in your project description requires **tuning and experimentation**.\n",
        "\n",
        "Here are the steps you would take to improve the score:\n",
        "\n",
        "1.  **Try a Different Model:** Naive Bayes is a good baseline, but other models often perform better on this task. Replace `MultinomialNB()` with one of these:\n",
        "      * `from sklearn.linear_model import LogisticRegression`\n",
        "        `model = LogisticRegression(max_iter=1000)`\n",
        "      * `from sklearn.svm import LinearSVC`\n",
        "        `model = LinearSVC()`\n",
        "2.  **Tune Hyperparameters:** You can use `GridSearchCV` from Scikit-learn to automatically find the best settings for your model and vectorizer (e.g., finding the optimal `max_features` for TF-IDF).\n",
        "3.  **Use a Better Dataset:** A larger, more balanced dataset (like the 1.6 million-tweet Sentiment140 dataset, though it's only positive/negative) can produce a more robust model."
      ],
      "metadata": {
        "id": "MC_ugg9Ln4jr"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}